# ===================== é…ç½®æ—¥å¿— =====================
import logging
# 1. è·å–PyTorchåˆ†å¸ƒå¼é‡å®šå‘æ¨¡å—çš„æ—¥å¿—å™¨
logger = logging.getLogger("torch.distributed.elastic.multiprocessing.redirects")
# 2. å°†è¯¥æ—¥å¿—å™¨çš„çº§åˆ«è®¾ä¸ºERRORï¼ˆä»…è¾“å‡ºé”™è¯¯ï¼Œå±è”½WARNINGåŠä»¥ä¸‹æ—¥å¿—ï¼‰
logger.setLevel(logging.ERROR)
# 3. ç¦æ­¢æ—¥å¿—ä¼ æ’­åˆ°çˆ¶æ—¥å¿—å™¨ï¼ˆé¿å…å…¶ä»–åœ°æ–¹é‡å¤è¾“å‡ºï¼‰
logger.propagate = False

# ===================== è­¦å‘Šè¿‡æ»¤ =====================
import warnings
warnings.filterwarnings(
    "ignore",
    message="NOTE: Redirects are currently not supported in Windows or MacOs.",
    category=UserWarning,
    module="torch.distributed.elastic.multiprocessing.redirects"
)

import torch
import torch.nn.functional as F
import os
import re
import sys # å¼•å…¥sysç”¨äºæŸ¥æ‰¾æ ¹ç›®å½•
from transformers import MegatronBertConfig, MegatronBertModel
from transformers import BertTokenizer

# ===================== å…¨å±€æ¨¡å‹å®¹å™¨å’Œé…ç½®ï¼ˆåˆå§‹ä¸ºç©ºï¼‰=====================
tokenizer = None
model = None
device = None
model_name = "" # æ–°å¢ï¼šç”¨äºå­˜å‚¨å½“å‰æ¨¡å‹åç§°ï¼Œä¾¿äºæ‰“å°
config = None


# ===================== æ¨¡å‹åˆå§‹åŒ–å‡½æ•°ï¼ˆä¼ å…¥æ¨¡å‹ç›¸å¯¹è·¯å¾„ï¼‰=====================
def init_model_and_tokenizer(model_dir, current_model_name):
    """
    åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨ï¼ˆå…¨å±€ä»…è°ƒç”¨1æ¬¡ï¼‰
    """
    global tokenizer, model, device, model_name, config # å£°æ˜å…¨å±€å˜é‡
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    if not os.path.isdir(model_dir):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ç›®å½•ä¸å­˜åœ¨ï¼š{model_dir}")
        print("è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å¤¹ç»“æ„æ˜¯å¦æ­£ç¡®")
        sys.exit(1)

    # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼ˆfloat16èŠ‚çœæ˜¾å­˜ï¼Œé€‚é…GPUï¼‰
    try:
        tokenizer = BertTokenizer.from_pretrained(model_dir)
        config = MegatronBertConfig.from_pretrained(model_dir)
        model = MegatronBertModel.from_pretrained(model_dir, dtype=torch.float16).to(device) # , dtype=torch.float16
        model.eval()  # å›ºå®šä¸ºæ¨ç†æ¨¡å¼ï¼Œé¿å…æ„å¤–è®­ç»ƒ
        model_name = current_model_name
        
        print("\n" + "="*50)
        print(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆ | ä½¿ç”¨è®¾å¤‡ï¼š{device}")
        print(f"ğŸ“Œ æ¨¡å‹ï¼š{model_name}ï¼ˆ{config.hidden_size}ç»´ç‰¹å¾ï¼‰")
        print("="*50)
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{e}")
        sys.exit(1)


# ===================== è¾…åŠ©å‡½æ•°ï¼ˆä»…è¯»å–æŒ‡å®šåˆ—ï¼‰=====================
def read_target_column_from_csv(file_path, target_column):
    """
    ä»CSVæ–‡ä»¶ä¸­ä»…è¯»å–æŒ‡å®šç›®æ ‡åˆ—ï¼ˆå¦‚â€œæ ‡é¢˜â€â€œæ¦‚è¦â€â€œæ‘˜è¦â€ï¼‰
    :param file_path: CSVæ–‡ä»¶è·¯å¾„
    :param target_column: ç›®æ ‡åˆ—åï¼ˆå¦‚â€œæ ‡é¢˜â€â€œæ¦‚è¦â€ï¼‰
    :return: ç›®æ ‡åˆ—çš„æœ‰æ•ˆæ–‡æœ¬åˆ—è¡¨
    """
    if not os.path.exists(file_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}")
        return []
    
    try:
        target_texts = []
        # ä¼˜å…ˆä½¿ç”¨pandasè¯»å–ï¼ˆæ›´ç¨³å®šçš„åˆ—è¯†åˆ«ï¼‰
        try:
            import pandas as pd
            df = pd.read_csv(file_path, encoding='utf-8-sig')
            
            # ä»…ä¿ç•™ç›®æ ‡åˆ—ï¼Œä¸å­˜åœ¨åˆ™æç¤º
            if target_column not in df.columns:
                print(f"âŒ CSVæ–‡ä»¶ä¸­æœªæ‰¾åˆ°â€œ{target_column}â€åˆ—")
                return []
            
            # æå–ç›®æ ‡åˆ—æœ‰æ•ˆæ–‡æœ¬ï¼ˆå»é‡ã€è¿‡æ»¤ç©ºå€¼å’Œè¿‡çŸ­æ–‡æœ¬ï¼‰
            for text in df[target_column].dropna().unique():
                text_str = str(text).strip()
                if len(text_str) > 20:  # è¿‡æ»¤è¿‡çŸ­æ–‡æœ¬ï¼ˆé¿å…æ— æ•ˆå†…å®¹ï¼‰
                    target_texts.append(text_str)
        
        except ImportError:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨csvæ¨¡å—è¯»å–
            import csv
            with open(file_path, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)  # æŒ‰åˆ—åè¯»å–
                if target_column not in reader.fieldnames:
                    print(f"âŒ CSVæ–‡ä»¶ä¸­æœªæ‰¾åˆ°â€œ{target_column}â€åˆ—")
                    return []
                
                for row in reader:
                    text_str = str(row[target_column]).strip()
                    if len(text_str) > 20:
                        target_texts.append(text_str)
        
        if not target_texts:
            print(f"âŒ CSVæ–‡ä»¶ä¸­â€œ{target_column}â€åˆ—æ— æœ‰æ•ˆæ–‡æœ¬")
            return []
        
        print(f"âœ… æˆåŠŸè¯»å– {len(target_texts)} æ¡{target_column}")
        return target_texts
    
    except Exception as e:
        print(f"âŒ è¯»å–CSVæ–‡ä»¶å¤±è´¥ï¼š{e}")
        return []


# ===================== æ–°å¢ï¼šé›¶æ ·æœ¬åˆ†ç±»è¾…åŠ©å‡½æ•° =====================
def predict_by_similarity(model, tokenizer, device, text_feat, label_map):
    """
    åŸç†ï¼šä¸è®­ç»ƒåˆ†ç±»å±‚ï¼Œè€Œæ˜¯è®¡ç®—[è¾“å…¥æ–‡æœ¬ç‰¹å¾]ä¸[ç±»åˆ«åç§°ç‰¹å¾]çš„ä½™å¼¦ç›¸ä¼¼åº¦
    """
    # 1. å‡†å¤‡ç±»åˆ«çš„æè¿°æ–‡æœ¬ï¼ˆPrompt Engineeringï¼‰
    # ä¾‹å¦‚æŠŠ "ä½“è‚²" æ‰©å±•ä¸º "è¿™æ˜¯ä¸€ä¸ªä½“è‚²ç±»åˆ«" ä»¥å¢åŠ è¯­ä¹‰åŒ¹é…åº¦
    label_texts = [f"å…³äº{label}çš„å†…å®¹" for label in label_map.values()]

    # 2. ç¼–ç æ‰€æœ‰ç±»åˆ«
    label_encoded = tokenizer(
        label_texts,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=32
    ).to(device)

    # 3. æå–ç±»åˆ«çš„ç‰¹å¾
    with torch.no_grad():
        label_outputs = model(** label_encoded)
        label_feats = label_outputs.pooler_output

    # 4. è®¡ç®—ç›¸ä¼¼åº¦ (Cosine Similarity)
    # å½’ä¸€åŒ–å‘é‡
    text_feat_norm = F.normalize(text_feat, p=2, dim=1).to(torch.float16) ##
    label_feats_norm = F.normalize(label_feats, p=2, dim=1).to(torch.float16) ##

    # çŸ©é˜µä¹˜æ³•è®¡ç®—ç›¸ä¼¼åº¦
    similarities = torch.mm(text_feat_norm, label_feats_norm.T)

    # 5. è·å–ç»“æœ
    # ä¹˜ä»¥ä¸€ä¸ªç¼©æ”¾å› å­(scale)è®©softmaxåˆ†å¸ƒæ›´å°–é”
    logits = similarities * 15
    pred_probs = F.softmax(logits, dim=1).cpu().numpy()[0]
    pred_idx = int(torch.argmax(logits).item())

    return pred_idx, pred_probs

# ===================== ä»»åŠ¡å‡½æ•°å°è£…ï¼ˆæ¯ä¸ªä»»åŠ¡ç‹¬ç«‹æˆå‡½æ•°ï¼‰=====================
def chidf_task(tokenizer, model, device):
    """ä»»åŠ¡1ï¼šCHIDFï¼ˆæˆè¯­å¡«ç©ºï¼‰"""
    global model_name
    print("\n" + "="*50)
    print(f"ğŸ“ ä»»åŠ¡1ï¼šæˆè¯­å¡«ç©ºï¼ˆCHIDFï¼‰ - æ¨¡å‹: {model_name}")
    print("="*50)
    
    # 1. è·å–ç”¨æˆ·è¾“å…¥ï¼ˆå«[MASK]çš„å¥å­ï¼‰
    user_text = input("è¯·è¾“å…¥å«æˆè¯­ç©ºç¼ºçš„å¥å­ï¼ˆç”¨[MASK]æ ‡è®°ç©ºç¼ºä½ç½®ï¼Œä¾‹å¦‚ï¼šä»–é¢å¯¹å›°éš¾æ—¶[MASK]ï¼‰ï¼š")
    if "[MASK]" not in user_text:
        print("âŒ è¾“å…¥é”™è¯¯ï¼šè¯·åœ¨å¥å­ä¸­æ·»åŠ [MASK]æ ‡è®°æˆè¯­ç©ºç¼ºä½ç½®ï¼")
        return
    
    # 2. ç¼–ç æ–‡æœ¬
    encoded = tokenizer(
        user_text,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=64
    ).to(device)
    mask_pos = torch.where(encoded["input_ids"] == tokenizer.mask_token_id)[1]
    if len(mask_pos) == 0:
        print("âŒ æœªè¯†åˆ«åˆ°[MASK]æ ‡è®°ï¼Œè¯·é‡æ–°è¾“å…¥ï¼")
        return
    
    # 3. æå–[MASK]ä½ç½®ç‰¹å¾
    with torch.no_grad():
        outputs = model(**encoded)
        mask_feat = outputs.last_hidden_state[0, mask_pos, :]
    
    # 4. è·å–ç”¨æˆ·æä¾›çš„å€™é€‰æˆè¯­
    candidate_input = input("è¯·è¾“å…¥å€™é€‰æˆè¯­ï¼ˆç”¨é€—å·åˆ†éš”ï¼Œä¾‹å¦‚ï¼šåšæŒä¸æ‡ˆï¼ŒåŠé€”è€ŒåºŸï¼‰ï¼š")
    if not candidate_input.strip():
        candidate_idioms = ["åšæŒä¸æ‡ˆ", "åŠé€”è€ŒåºŸ", "ç•ç¼©ä¸å‰", "æ•·è¡äº†äº‹"]
        print(f"âš ï¸ æœªè¾“å…¥å€™é€‰æˆè¯­ï¼Œä½¿ç”¨é»˜è®¤å€™é€‰ï¼š{','.join(candidate_idioms)}")
    else:
        # ä½¿ç”¨å…¨è§’é€—å·åˆ†éš”ï¼Œå’ŒåŸå§‹ä»£ç ä¿æŒä¸€è‡´
        candidate_idioms = [idiom.strip() for idiom in candidate_input.split("ï¼Œ")]
    
    # 5. æ”¹è¿›çš„ç‰¹å¾æå–ï¼šå°†æˆè¯­æ”¾å…¥ç›¸åŒä¸Šä¸‹æ–‡ä¸­è·å–ç‰¹å¾
    print(f"\nğŸ”„ æ­£åœ¨è®¡ç®—{len(candidate_idioms)}ä¸ªå€™é€‰æˆè¯­çš„åŒ¹é…åº¦...")
    candidate_feats = []
    
    for idiom in candidate_idioms:
        # å°†æˆè¯­æ”¾å…¥ä¸åŸå§‹å¥å­ç›¸ä¼¼çš„ä¸Šä¸‹æ–‡ä¸­
        template_text = f"è¿™ä¸ªæˆè¯­çš„æ„æ€æ˜¯ï¼š{idiom}"
        idiom_encoded = tokenizer(
            template_text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=32
        ).to(device)
        
        with torch.no_grad():
            idiom_output = model(** idiom_encoded)
            # ä½¿ç”¨[CLS] tokençš„ç‰¹å¾ï¼Œä¸mask_featä¿æŒä¸€è‡´
            idiom_feat = idiom_output.last_hidden_state[0, 0, :]  # [CLS] token
            candidate_feats.append(idiom_feat)
    
    # 6. è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆæ·»åŠ ç‰¹å¾å½’ä¸€åŒ–ï¼‰
    candidate_feats = torch.stack(candidate_feats, dim=0)
    
    # ç‰¹å¾å½’ä¸€åŒ–ï¼Œæé«˜ç›¸ä¼¼åº¦è®¡ç®—ç¨³å®šæ€§
    mask_feat_norm = F.normalize(mask_feat, p=2, dim=1)
    candidate_feats_norm = F.normalize(candidate_feats, p=2, dim=1)
    
    similarities = F.cosine_similarity(mask_feat_norm, candidate_feats_norm, dim=1)
    
    # 7. å¦‚æœæ‰€æœ‰ç›¸ä¼¼åº¦éƒ½å¾ˆä½ï¼Œå°è¯•å¦ä¸€ç§ç‰¹å¾æå–æ–¹å¼
    if torch.max(similarities) < 0.1:
        print("âš ï¸  æ£€æµ‹åˆ°åŒ¹é…åº¦è¾ƒä½ï¼Œå°è¯•å¤‡ç”¨ç‰¹å¾æå–æ–¹æ³•...")
        candidate_feats_alt = []
        
        for idiom in candidate_idioms:
            # å¤‡ç”¨æ–¹æ³•ï¼šç›´æ¥ç¼–ç æˆè¯­ï¼Œä½¿ç”¨å¹³å‡æ± åŒ–
            idiom_encoded = tokenizer(
                idiom,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=8
            ).to(device)
            
            with torch.no_grad():
                idiom_output = model(**idiom_encoded)
                # ä½¿ç”¨æ‰€æœ‰tokençš„å¹³å‡ç‰¹å¾
                mean_feat = torch.mean(idiom_output.last_hidden_state[0], dim=0)
                candidate_feats_alt.append(mean_feat)
        
        candidate_feats_alt = torch.stack(candidate_feats_alt, dim=0)
        candidate_feats_alt_norm = F.normalize(candidate_feats_alt, p=2, dim=1)
        similarities = F.cosine_similarity(mask_feat_norm, candidate_feats_alt_norm, dim=1)
    
    # 8. é€‰æ‹©æœ€åŒ¹é…çš„æˆè¯­
    best_idx = torch.argmax(similarities).item()
    
    # 9. è¾“å‡ºç»“æœ
    print("\n" + "="*30 + " åŒ¹é…ç»“æœ " + "="*30)
    print(f"ğŸ“– åŸå¥ï¼š{user_text}")
    print(f"ğŸ† æœ€ä½³åŒ¹é…æˆè¯­ï¼š{candidate_idioms[best_idx]}")
    print(f"ğŸ“Š åŒ¹é…ç½®ä¿¡åº¦ï¼š{similarities[best_idx].item():.4f}")
    
    # å°†ç›¸ä¼¼åº¦æ˜ å°„åˆ°[0,1]èŒƒå›´æ˜¾ç¤º
    normalized_similarities = (similarities + 1) / 2  # ä»[-1,1]æ˜ å°„åˆ°[0,1]
    
    print(f"\nğŸ“‹ æ‰€æœ‰å€™é€‰æˆè¯­åŒ¹é…åº¦æ’åï¼š")
    sorted_pairs = sorted(zip(candidate_idioms, normalized_similarities.cpu().numpy(), similarities.cpu().numpy()), 
                         key=lambda x: x[1], reverse=True)
    for i, (idiom, norm_sim, raw_sim) in enumerate(sorted_pairs, 1):
        print(f"  {i}. {idiom} â†’ åŒ¹é…åº¦ï¼š{norm_sim:.4f} (åŸå§‹ï¼š{raw_sim:.4f})")


def tnews_task(tokenizer, model, device):
    """ä»»åŠ¡2ï¼šTNEWSï¼ˆæ–°é—»åˆ†ç±»ï¼‰"""
    global model_name
    print("\n" + "="*50)
    print(f"ğŸ“° ä»»åŠ¡2ï¼šæ–°é—»åˆ†ç±»ï¼ˆTNEWSï¼‰ - æ¨¡å‹: {model_name}")
    print("="*50)
    
    # 1. å®šä¹‰æ–°é—»ç±»åˆ«ï¼ˆCLUEåŸºå‡†15ç±»å®Œæ•´ç‰ˆï¼‰
    news_labels = {
        0: "ç§‘æŠ€", 1: "å¨±ä¹", 2: "ä½“è‚²", 3: "è´¢ç»", 4: "æ—¶æ”¿", 5: "æ•™è‚²",
        6: "å†›äº‹", 7: "æ±½è½¦", 8: "æˆ¿äº§", 9: "æ¸¸æˆ", 10: "æ—¶å°š", 11: "å½©ç¥¨",
        12: "è‚¡ç¥¨", 13: "å®¶å±…", 14: "ç¤¾ä¼š"
    }
    print(f"æ”¯æŒåˆ†ç±»ï¼š{', '.join([f'{k}:{v}' for k, v in news_labels.items()])}")
    
    # 2. è·å–ç”¨æˆ·è¾“å…¥æ–¹å¼
    print("\nè¯·é€‰æ‹©æ–°é—»è¾“å…¥æ–¹å¼ï¼š")
    print("1. æ‰‹åŠ¨è¾“å…¥å•æ¡æ–°é—»")
    print("2. ä»Information/Newsæ–‡ä»¶å¤¹è¯»å–CSVæ–‡ä»¶è¿›è¡Œæ‰¹é‡åˆ†ç±»")
    input_choice = input("è¯·è¾“å…¥é€‰é¡¹ç¼–å·ï¼ˆ1/2ï¼‰ï¼š")
    
    news_texts = []
    if input_choice == "1":
        # æ‰‹åŠ¨è¾“å…¥å•æ¡æ–°é—»
        user_news = input("è¯·è¾“å…¥éœ€è¦åˆ†ç±»çš„æ–°é—»æ–‡æœ¬ï¼š")
        if not user_news.strip():
            print("âŒ è¾“å…¥é”™è¯¯ï¼šæ–°é—»æ–‡æœ¬ä¸èƒ½ä¸ºç©ºï¼")
            return
        news_texts = [user_news]
        
    elif input_choice == "2":
        # ä»Newsæ–‡ä»¶å¤¹è¯»å–CSVæ–‡ä»¶è¿›è¡Œæ‰¹é‡åˆ†ç±»
        # è„šæœ¬ç°åœ¨åœ¨ 'æ¨¡å‹' æ–‡ä»¶å¤¹ï¼ŒNewsåœ¨ 'æ¨¡å‹/Information/News'
        script_dir = os.path.dirname(os.path.abspath(__file__))
        news_folder = os.path.join(script_dir, "Spiders", "Information", "News")
        
        # æ£€æŸ¥Newsæ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
        if not os.path.exists(news_folder):
            print(f"âŒ Newsæ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼š{news_folder}")
            print("ğŸ’¡ è¯·ç¡®ä¿Information/Newsæ–‡ä»¶å¤¹ç»“æ„æ­£ç¡®")
            return
        
        # è·å–Newsæ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰CSVæ–‡ä»¶
        csv_files = [f for f in os.listdir(news_folder) if f.endswith('.csv')]
        if not csv_files:
            print(f"âŒ Newsæ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ°CSVæ–‡ä»¶")
            return
        
        # æ˜¾ç¤ºå¯ç”¨çš„CSVæ–‡ä»¶
        print(f"\nğŸ“ æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶ï¼š")
        for i, csv_file in enumerate(csv_files, 1):
            print(f"  {i}. {csv_file}")
        
        # è®©ç”¨æˆ·é€‰æ‹©æ–‡ä»¶
        try:
            file_choice = input(f"\nè¯·é€‰æ‹©æ–‡ä»¶ç¼–å·ï¼ˆ1-{len(csv_files)}ï¼‰ï¼š").strip()
            file_index = int(file_choice) - 1
            if file_index < 0 or file_index >= len(csv_files):
                print("âŒ è¾“å…¥é”™è¯¯ï¼šç¼–å·è¶…å‡ºèŒƒå›´ï¼")
                return
            
            selected_file = csv_files[file_index]
            file_path = os.path.join(news_folder, selected_file)
            print(f"âœ… é€‰æ‹©æ–‡ä»¶ï¼š{selected_file}")
            
            # ä½¿ç”¨ç»Ÿä¸€å·¥å…·å‡½æ•°è¯»å–"æ ‡é¢˜"åˆ—
            news_texts = read_target_column_from_csv(file_path, "æ ‡é¢˜")
            if not news_texts:
                return
                
        except ValueError:
            print("âŒ è¾“å…¥é”™è¯¯ï¼šè¯·è¾“å…¥æ•°å­—ç¼–å·ï¼")
            return
        
    else:
        print("âŒ è¾“å…¥é”™è¯¯ï¼è¯·è¾“å…¥1æˆ–2é€‰æ‹©è¾“å…¥æ–¹å¼")
        return
    
    # 3. è¿›è¡Œåˆ†ç±»é¢„æµ‹
    print(f"\nğŸ” å¼€å§‹å¯¹ {len(news_texts)} æ¡æ–°é—»è¿›è¡Œåˆ†ç±»...")
    
    results = []
    for i, news_text in enumerate(news_texts, 1):
        if input_choice == "2":  # æ‰¹é‡å¤„ç†æ—¶æ˜¾ç¤ºè¿›åº¦
            print(f"ğŸ“Š å¤„ç†è¿›åº¦ï¼š{i}/{len(news_texts)}", end="\r")
        
        # ç¼–ç æ–‡æœ¬å¹¶æå–ç‰¹å¾
        encoded = tokenizer(
            news_text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=128  # å¢åŠ é•¿åº¦ä»¥é€‚åº”æ–°é—»æ–‡æœ¬
        ).to(device)
        
        with torch.no_grad():
            outputs = model(**encoded)
            feat = outputs.pooler_output.to(torch.float16) ##

            # ä½¿ç”¨ç›¸ä¼¼åº¦åŒ¹é…ä»£æ›¿éšæœºåˆ†ç±»å¤´
            pred_label_idx, pred_probs = predict_by_similarity(model, tokenizer, device, feat, news_labels)
            pred_label = news_labels[pred_label_idx]
            confidence = pred_probs[pred_label_idx]
        
        results.append({
            'text': news_text,
            'pred_label': pred_label,
            'pred_idx': pred_label_idx,
            'confidence': confidence,
            'all_probs': pred_probs
        })
    
    # 4. è¾“å‡ºç»“æœ
    print("\n" + "="*40 + " åˆ†ç±»ç»“æœ " + "="*40)
    
    if input_choice == "1":
        # å•æ¡æ–°é—»è¯¦ç»†ç»“æœ
        result = results[0]
        print(f"ğŸ“ æ–°é—»æ–‡æœ¬ï¼š{result['text']}")
        print(f"ğŸ·ï¸ é¢„æµ‹ç±»åˆ«ï¼š{result['pred_label']}ï¼ˆç±»åˆ«ç¼–å·ï¼š{result['pred_idx']}ï¼‰")
        print(f"ğŸ“Š ç½®ä¿¡åº¦ï¼š{result['confidence']:.4f}")
        print(f"ğŸ“ˆ å„ç±»åˆ«æ¦‚ç‡ï¼š")
        for label_idx, label_name in news_labels.items():
            prob = result['all_probs'][label_idx]
            print(f"  {label_name}ï¼š{prob:.4f}")
    
    else:
        # æ‰¹é‡ç»“æœæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print(f"ğŸ“ˆ æ‰¹é‡åˆ†ç±»ç»Ÿè®¡ï¼š")
        print(f"   æ€»æ–°é—»æ•°ï¼š{len(results)}")
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        from collections import Counter
        label_counts = Counter([r['pred_label'] for r in results])
        print(f"\nğŸ“Š ç±»åˆ«åˆ†å¸ƒï¼š")
        for label in news_labels.values():
            count = label_counts.get(label, 0)
            percentage = (count / len(results)) * 100
            print(f"  {label}ï¼š{count}æ¡ ({percentage:.1f}%)")
        
        # æ˜¾ç¤ºå‰å‡ æ¡ç»“æœçš„è¯¦æƒ…
        print(f"\nğŸ” å‰{min(5, len(results))}æ¡æ–°é—»è¯¦æƒ…ï¼š")
        for i, result in enumerate(results[:5], 1):
            print(f"\n{i}. {result['text'][:100]}...")
            print(f"   ç±»åˆ«ï¼š{result['pred_label']} | ç½®ä¿¡åº¦ï¼š{result['confidence']:.4f}")
        
        # è¯¢é—®æ˜¯å¦æ˜¾ç¤ºæ‰€æœ‰ç»“æœ
        if len(results) > 5:
            show_all = input(f"\nğŸ’¡ è¿˜æœ‰{len(results)-5}æ¡ç»“æœæœªæ˜¾ç¤ºï¼Œæ˜¯å¦æ˜¾ç¤ºå…¨éƒ¨ï¼Ÿ(y/n)ï¼š")
            if show_all.lower() == 'y':
                for i, result in enumerate(results, 1):
                    print(f"\n{i}. {result['text']}")
                    print(f"   ç±»åˆ«ï¼š{result['pred_label']} | ç½®ä¿¡åº¦ï¼š{result['confidence']:.4f}")


def ocnli_task(tokenizer, model, device): # ç§»é™¤configå‚æ•°ï¼Œç›´æ¥ä½¿ç”¨å…¨å±€config
    """ä»»åŠ¡3ï¼šOCNLIï¼ˆè‡ªç„¶è¯­è¨€æ¨ç†ï¼‰"""
    global model_name, config
    print("\n" + "="*50)
    print(f"ğŸ” ä»»åŠ¡3ï¼šè‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆOCNLIï¼‰ - æ¨¡å‹: {model_name}")
    print("="*50)
    
    # 1. å®šä¹‰æ¨ç†ç±»åˆ«
    nli_labels = {0: "è•´å«", 1: "çŸ›ç›¾", 2: "ä¸­ç«‹"}
    print(f"æ¨ç†å…³ç³»ï¼š{', '.join([f'{k}:{v}' for k, v in nli_labels.items()])}")
    print("ç¤ºä¾‹ï¼šå¥å­1='äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•å¿«'ï¼Œå¥å­2='AIæŠ€æœ¯è¿­ä»£å¿«' â†’ è•´å«å…³ç³»")
    
    # 2. è·å–ç”¨æˆ·è¾“å…¥çš„å¥å­å¯¹
    sent1 = input("è¯·è¾“å…¥å¥å­1ï¼ˆå‰æï¼‰ï¼š")
    sent2 = input("è¯·è¾“å…¥å¥å­2ï¼ˆå‡è®¾ï¼‰ï¼š")
    if not sent1.strip() or not sent2.strip():
        print("âŒ è¾“å…¥é”™è¯¯ï¼šå¥å­1å’Œå¥å­2ä¸èƒ½ä¸ºç©ºï¼")
        return
    
    # 3. ç¼–ç å¥å­å¯¹ï¼ˆç”¨[SEP]åˆ†éš”ï¼‰
    nli_text = f"{sent1} {tokenizer.sep_token} {sent2}"
    encoded = tokenizer(
        nli_text,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=128
    ).to(device)
    
    # 4. æå–ç‰¹å¾å¹¶æ¨ç†
    with torch.no_grad():
        outputs = model(**encoded)
        feat = outputs.pooler_output.to(torch.float16) ##
    
    # 5. æ¨ç†åˆ†ç±»å¤´
    # åŠ¨æ€ä½¿ç”¨å…¨å±€ config
    input_size = config.hidden_size 
    # æ³¨æ„ï¼šè¿™ä¸ªåˆ†ç±»å¤´æ˜¯éšæœºåˆå§‹åŒ–çš„ï¼Œå¯¹äºé›¶æ ·æœ¬æ•ˆæœå¯èƒ½ä¸ä½³ï¼Œä½†ä¿æŒåŸé€»è¾‘
    nli_classifier = torch.nn.Linear(input_size, 3).to(device, dtype=torch.float16)
    with torch.no_grad():
        logits = nli_classifier(feat)
        pred_probs = F.softmax(logits, dim=1).cpu().numpy()[0]
        pred_label_idx = int(torch.argmax(logits).item())
        pred_relation = nli_labels[pred_label_idx]

    # 6. è¾“å‡ºç»“æœ
    print("\n" + "="*30 + " ç»“æœ " + "="*30)
    print(f"å‰æå¥å­ï¼š{sent1}")
    print(f"å‡è®¾å¥å­ï¼š{sent2}")
    print(f"æ¨ç†å…³ç³»ï¼š{pred_relation}ï¼ˆç½®ä¿¡åº¦ï¼š{pred_probs[pred_label_idx]:.4f}ï¼‰")
    print(f"å„å…³ç³»ç½®ä¿¡åº¦ï¼š{dict(zip(nli_labels.values(), pred_probs.round(4)))}")


def csl_task(tokenizer, model, device):
    """ä»»åŠ¡4ï¼šCSLï¼ˆæ‘˜è¦å…³é”®è¯è¯†åˆ«ï¼‰- å…³é”®è¯çœŸå®æ€§éªŒè¯"""
    global model_name
    print("\n" + "="*50)
    print(f"ğŸ”¤ ä»»åŠ¡4ï¼šæ‘˜è¦å…³é”®è¯çœŸå®æ€§éªŒè¯ï¼ˆCSLï¼‰ - æ¨¡å‹: {model_name}")
    print("="*50)
    print("ä»»åŠ¡ç›®æ ‡ï¼šåˆ¤æ–­å€™é€‰å…³é”®è¯æ˜¯å¦å‡†ç¡®åæ˜ å­¦æœ¯è®ºæ–‡æ‘˜è¦çš„æ ¸å¿ƒå†…å®¹")
    print("è¾“å‡ºç»“æœï¼š1ï¼ˆå‡†ç¡®ï¼‰/ 0ï¼ˆä¸å‡†ç¡®ï¼‰")
    
    # 1. é€‰æ‹©è¾“å…¥æ–¹å¼
    print("\nè¯·é€‰æ‹©è¾“å…¥æ–¹å¼ï¼š")
    print("1. æ‰‹åŠ¨è¾“å…¥ï¼ˆæ‘˜è¦+å€™é€‰å…³é”®è¯ï¼‰")
    print("2. ä»Information/CSLæ–‡ä»¶è¯»å–ï¼ˆéœ€åŒ…å«'æ‘˜è¦'å’Œ'å…³é”®è¯'åˆ—ï¼‰")
    input_choice = input("è¯·è¾“å…¥é€‰é¡¹ç¼–å·ï¼ˆ1/2ï¼‰ï¼š")
    
    data_list = []
    if input_choice == "1":
        # æ‰‹åŠ¨è¾“å…¥æ¨¡å¼
        print("\n" + "-"*30 + " æ‰‹åŠ¨è¾“å…¥ " + "-"*30)
        # è·å–æ‘˜è¦
        user_abstract = input("è¯·è¾“å…¥å­¦æœ¯è®ºæ–‡æ‘˜è¦ï¼š")
        if not user_abstract.strip():
            print("âŒ è¾“å…¥é”™è¯¯ï¼šæ‘˜è¦æ–‡æœ¬ä¸èƒ½ä¸ºç©ºï¼")
            return
        # è·å–å€™é€‰å…³é”®è¯
        user_keywords = input("è¯·è¾“å…¥å€™é€‰å…³é”®è¯ï¼ˆç”¨é€—å·åˆ†éš”ï¼Œä¾‹å¦‚ï¼šäººå·¥æ™ºèƒ½,æ·±åº¦å­¦ä¹ ,ç¥ç»ç½‘ç»œï¼‰ï¼š")
        if not user_keywords.strip():
            print("âŒ è¾“å…¥é”™è¯¯ï¼šå…³é”®è¯ä¸èƒ½ä¸ºç©ºï¼")
            return
        # å¤„ç†å…³é”®è¯åˆ—è¡¨
        candidate_keywords = [kw.strip() for kw in re.split(r"[,ï¼Œ]", user_keywords) if kw.strip()]
        if not candidate_keywords:
            print("âŒ è¾“å…¥é”™è¯¯ï¼šæœªè¯†åˆ«åˆ°æœ‰æ•ˆå…³é”®è¯ï¼")
            return
        
        # æ„é€ å•æ¡æ•°æ®
        data_list = [{"abstract": user_abstract, "keywords": candidate_keywords}]
    
    elif input_choice == "2":
        # CSVè¯»å–æ¨¡å¼
        print("\n" + "-"*30 + " CSVæ–‡ä»¶è¯»å– " + "-"*30)
        script_dir = os.path.dirname(os.path.abspath(__file__))
        # é¢„è®¾CSVæ–‡ä»¶å­˜æ”¾è·¯å¾„ï¼ˆä¸è„šæœ¬åŒç›®å½•çš„Information/CSLæ–‡ä»¶å¤¹ï¼‰
        csl_folder = os.path.join(script_dir, "Spiders", "Information", "Literature")
        
        if not os.path.exists(csl_folder):
            os.makedirs(csl_folder)
            print(f"âš ï¸ å·²è‡ªåŠ¨åˆ›å»ºCSLæ–‡ä»¶å¤¹ï¼š{csl_folder}")
            print("ğŸ’¡ è¯·å°†åŒ…å«'æ‘˜è¦'å’Œ'å…³é”®è¯'åˆ—çš„CSVæ–‡ä»¶æ”¾å…¥è¯¥æ–‡ä»¶å¤¹åé‡è¯•")
            return
        
        # è·å–æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰CSVæ–‡ä»¶
        csv_files = [f for f in os.listdir(csl_folder) if f.endswith('.csv')]
        if not csv_files:
            print(f"âŒ CSLæ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ°CSVæ–‡ä»¶ï¼š{csl_folder}")
            return
        
        # é€‰æ‹©CSVæ–‡ä»¶
        print(f"\nğŸ“ æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶ï¼š")
        for i, csv_file in enumerate(csv_files, 1):
            print(f"  {i}. {csv_file}")
        
        try:
            file_index = int(input(f"\nè¯·é€‰æ‹©æ–‡ä»¶ç¼–å·ï¼ˆ1-{len(csv_files)}ï¼‰ï¼š")) - 1
            if file_index < 0 or file_index >= len(csv_files):
                print("âŒ è¾“å…¥é”™è¯¯ï¼šç¼–å·è¶…å‡ºèŒƒå›´ï¼")
                return
            selected_file = os.path.join(csl_folder, csv_files[file_index])
            print(f"âœ… é€‰æ‹©æ–‡ä»¶ï¼š{csv_files[file_index]}")
            
            # ä½¿ç”¨ç»Ÿä¸€å·¥å…·å‡½æ•°åˆ†åˆ«è¯»å–"æ‘˜è¦"å’Œ"å…³é”®è¯"åˆ—
            abstracts = read_target_column_from_csv(selected_file, "æ‘˜è¦")
            # å…³é”®è¯åˆ—æ— éœ€è¿‡æ»¤è¿‡çŸ­æ–‡æœ¬ï¼ˆ>20ï¼‰ï¼Œä½†read_target_column_from_csvåŒ…å«äº†æ­¤é€»è¾‘ï¼Œè¿™é‡Œä¿æŒåŸæ ·
            keywords_list = read_target_column_from_csv(selected_file, "å…³é”®è¯") 
            
            if not abstracts or not keywords_list:
                return
                
            # ç¡®ä¿æ•°æ®é•¿åº¦ä¸€è‡´
            min_len = min(len(abstracts), len(keywords_list))
            data_list = []
            for i in range(min_len):
                # å…¼å®¹æ—§ä»£ç å¤„ç†é€—å·
                keywords = [kw.strip() for kw in keywords_list[i].replace("ï¼›", "ï¼Œ").split("ï¼Œ") if kw.strip()]
                if keywords:
                    data_list.append({
                        "abstract": abstracts[i],
                        "keywords": keywords
                    })
            
            if not data_list:
                print("âŒ æœªæå–åˆ°æœ‰æ•ˆæ‘˜è¦-å…³é”®è¯æ•°æ®å¯¹")
                return
                
            print(f"âœ… æˆåŠŸè¯»å– {len(data_list)} æ¡æœ‰æ•ˆæ•°æ®")
        
        except ValueError:
            print("âŒ è¾“å…¥é”™è¯¯ï¼šè¯·è¾“å…¥æ•°å­—ç¼–å·ï¼")
            return
    
    else:
        print("âŒ è¾“å…¥é”™è¯¯ï¼šè¯·é€‰æ‹©1æˆ–2ï¼")
        return
    
    # 2. æ ¸å¿ƒéªŒè¯é€»è¾‘ï¼ˆè®¡ç®—æ‘˜è¦ä¸å…³é”®è¯çš„è¯­ä¹‰åŒ¹é…åº¦ï¼‰
    print(f"\nğŸ” å¼€å§‹éªŒè¯ {len(data_list)} æ¡æ•°æ®...")
    results = []
    
    for idx, data in enumerate(data_list, 1):
        abstract = data["abstract"]
        keywords = data["keywords"]
        
        # ç¼–ç æ‘˜è¦
        abstract_encoded = tokenizer(
            abstract,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=256
        ).to(device)
        
        # ç¼–ç å…³é”®è¯ï¼ˆæ‹¼æ¥ä¸ºä¸€å¥è¯ï¼‰
        keywords_text = "ï¼Œ".join(keywords)
        keywords_encoded = tokenizer(
            keywords_text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=64
        ).to(device)
        
        # æå–ç‰¹å¾
        with torch.no_grad():
            abstract_output = model(**abstract_encoded)
            abstract_feat = abstract_output.pooler_output
            
            keywords_output = model(** keywords_encoded)
            keywords_feat = keywords_output.pooler_output
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆå½’ä¸€åŒ–åï¼‰
        abstract_feat_norm = F.normalize(abstract_feat, p=2, dim=1)
        keywords_feat_norm = F.normalize(keywords_feat, p=2, dim=1)
        similarity = torch.mm(abstract_feat_norm, keywords_feat_norm.T).item()
        
        # äºŒåˆ†ç±»åˆ¤æ–­ï¼ˆé˜ˆå€¼è®¾ä¸º0.3ï¼Œå¯æ ¹æ®å®é™…æ•ˆæœè°ƒæ•´ï¼‰
        is_accurate = 1 if similarity >= 0.3 else 0
        results.append({
            "index": idx,
            "abstract": abstract,
            "keywords": keywords,
            "similarity": round(similarity, 4),
            "is_accurate": is_accurate
        })
    
    # 3. è¾“å‡ºç»“æœ
    print("\n" + "="*40 + " éªŒè¯ç»“æœ " + "="*40)
    for res in results:
        print(f"\nğŸ“ ç¬¬{res['index']}æ¡æ•°æ®ï¼š")
        print(f"æ‘˜è¦ï¼š{res['abstract'][:150]}..." if len(res['abstract']) > 150 else f"æ‘˜è¦ï¼š{res['abstract']}")
        print(f"å€™é€‰å…³é”®è¯ï¼š{','.join(res['keywords'])}")
        print(f"è¯­ä¹‰ç›¸ä¼¼åº¦ï¼š{res['similarity']}")
        print(f"éªŒè¯ç»“æœï¼š{'âœ… å‡†ç¡®ï¼ˆ1ï¼‰' if res['is_accurate'] == 1 else 'âŒ ä¸å‡†ç¡®ï¼ˆ0ï¼‰'}")


def csldcp_task(tokenizer, model, device):
    """ä»»åŠ¡5ï¼šCSLDCPï¼ˆä¸»é¢˜æ–‡çŒ®åˆ†ç±»ï¼‰"""
    global model_name
    print("\n" + "="*50)
    print(f"ğŸ“š ä»»åŠ¡5ï¼šä¸»é¢˜æ–‡çŒ®åˆ†ç±»ï¼ˆCSLDCPï¼‰ - æ¨¡å‹: {model_name}")
    print("="*50)
    
    # 1. å®šä¹‰ç»†ç²’åº¦å­¦ç§‘ç±»åˆ«ï¼ˆCLUEåŸºå‡†67ç±»å®Œæ•´ç‰ˆï¼‰
    csldcp_labels = {
        0: "è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯", 1: "ç”µå­ç§‘å­¦ä¸æŠ€æœ¯", 2: "ä¿¡æ¯ä¸é€šä¿¡å·¥ç¨‹", 3: "æ§åˆ¶ç§‘å­¦ä¸å·¥ç¨‹",
        4: "è½¯ä»¶å·¥ç¨‹", 5: "ç½‘ç»œç©ºé—´å®‰å…¨", 6: "æ•°å­¦", 7: "ç‰©ç†å­¦", 8: "åŒ–å­¦", 9: "ç”Ÿç‰©å­¦",
        10: "åŒ»å­¦", 11: "ç®¡ç†å­¦", 12: "ç»æµå­¦", 13: "æ³•å­¦", 14: "æ•™è‚²å­¦", 15: "æ–‡å­¦",
        16: "å†å²å­¦", 17: "å“²å­¦", 18: "è‰ºæœ¯å­¦", 19: "å†œå­¦", 20: "å·¥å­¦", 21: "ç†å­¦",
        22: "åŒ»å­¦æŠ€æœ¯", 23: "å…¬å…±å«ç”Ÿä¸é¢„é˜²åŒ»å­¦", 24: "è¯å­¦", 25: "ä¸­è¯å­¦", 26: "å£è…”åŒ»å­¦",
        27: "ä¸´åºŠåŒ»å­¦", 28: "æŠ¤ç†å­¦", 29: "åŸºç¡€åŒ»å­¦", 30: "ä¸­åŒ»å­¦", 31: "ä¸­è¥¿åŒ»ç»“åˆ",
        32: "ç®¡ç†ç§‘å­¦ä¸å·¥ç¨‹", 33: "å·¥å•†ç®¡ç†", 34: "å…¬å…±ç®¡ç†", 35: "å›¾ä¹¦æƒ…æŠ¥ä¸æ¡£æ¡ˆç®¡ç†",
        36: "åº”ç”¨ç»æµå­¦", 37: "ç†è®ºç»æµå­¦", 38: "ç»Ÿè®¡å­¦", 39: "æ³•å­¦ç†è®º", 40: "å®ªæ³•å­¦ä¸è¡Œæ”¿æ³•å­¦",
        41: "åˆ‘æ³•å­¦", 42: "æ°‘å•†æ³•å­¦", 43: "è¯‰è®¼æ³•å­¦", 44: "ç»æµæ³•å­¦", 45: "ç¯å¢ƒä¸èµ„æºä¿æŠ¤æ³•å­¦",
        46: "å›½é™…æ³•å­¦", 47: "å†›äº‹æ³•å­¦", 48: "æ•™è‚²å­¦åŸç†", 49: "è¯¾ç¨‹ä¸æ•™å­¦è®º", 50: "å­¦å‰æ•™è‚²å­¦",
        51: "é«˜ç­‰æ•™è‚²å­¦", 52: "æˆäººæ•™è‚²å­¦", 53: "èŒä¸šæŠ€æœ¯æ•™è‚²å­¦", 54: "ç‰¹æ®Šæ•™è‚²å­¦", 55: "æ•™è‚²æŠ€æœ¯å­¦",
        56: "ä¸­å›½è¯­è¨€æ–‡å­¦", 57: "å¤–å›½è¯­è¨€æ–‡å­¦", 58: "æ–°é—»ä¼ æ’­å­¦", 59: "è‰ºæœ¯å­¦ç†è®º", 60: "éŸ³ä¹ä¸èˆè¹ˆå­¦",
        61: "æˆå‰§ä¸å½±è§†å­¦", 62: "ç¾æœ¯å­¦", 63: "è®¾è®¡å­¦", 64: "å†å²å­¦ç†è®º", 65: "ä¸­å›½å²",
        66: "ä¸–ç•Œå²"
    }
    print(f"æ”¯æŒåˆ†ç±»ï¼šå…±{len(csldcp_labels)}ä¸ªç»†ç²’åº¦å­¦ç§‘ç±»åˆ«")
    print("ç¤ºä¾‹ç±»åˆ«ï¼šè®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯ã€ç”µå­ç§‘å­¦ä¸æŠ€æœ¯ã€æ•°å­¦ã€åŒ»å­¦ç­‰")
    
    # 2. è·å–ç”¨æˆ·è¾“å…¥æ–¹å¼ï¼ˆå•æ¡/æ‰¹é‡ï¼‰
    print("\nè¯·é€‰æ‹©è¾“å…¥æ–¹å¼ï¼š")
    print("1. æ‰‹åŠ¨è¾“å…¥å•ç¯‡æ–‡çŒ®æ‘˜è¦")
    print("2. ä»Information/Literatureæ–‡ä»¶å¤¹è¯»å–CSVæ–‡ä»¶æ‰¹é‡åˆ†ç±»")
    input_choice = input("è¯·è¾“å…¥é€‰é¡¹ç¼–å·ï¼ˆ1/2ï¼‰ï¼š")
    
    abstracts = []
    if input_choice == "1":
        # å•æ¡è¾“å…¥
        user_abstract = input("è¯·è¾“å…¥æ–‡çŒ®æ‘˜è¦æ–‡æœ¬ï¼š")
        if not user_abstract.strip():
            print("âŒ è¾“å…¥é”™è¯¯ï¼šæ‘˜è¦æ–‡æœ¬ä¸èƒ½ä¸ºç©ºï¼")
            return
        abstracts = [user_abstract]
    
    elif input_choice == "2":
        # æ‰¹é‡è¯»å–CSV
        script_dir = os.path.dirname(os.path.abspath(__file__))
        lit_folder = os.path.join(script_dir, "Spiders", "Information", "Literature")
        if not os.path.exists(lit_folder):
            print(f"âŒ Literatureæ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼š{lit_folder}")
            print("ğŸ’¡ è¯·ç¡®ä¿Information/Literatureæ–‡ä»¶å¤¹ç»“æ„æ­£ç¡®")
            return
        
        csv_files = [f for f in os.listdir(lit_folder) if f.endswith('.csv')]
        if not csv_files:
            print("âŒ Literatureæ–‡ä»¶å¤¹ä¸­æ— CSVæ–‡ä»¶")
            return
        
        # é€‰æ‹©CSVæ–‡ä»¶
        print(f"\nğŸ“ æ‰¾åˆ°{len(csv_files)}ä¸ªCSVæ–‡ä»¶ï¼š")
        for i, csv_file in enumerate(csv_files, 1):
            print(f"  {i}. {csv_file}")
        
        try:
            file_index = int(input(f"è¯·é€‰æ‹©æ–‡ä»¶ç¼–å·ï¼ˆ1-{len(csv_files)}ï¼‰ï¼š")) - 1
            if file_index < 0 or file_index >= len(csv_files):
                print("âŒ ç¼–å·è¶…å‡ºèŒƒå›´ï¼")
                return
            selected_file = os.path.join(lit_folder, csv_files[file_index])
            
            # ä½¿ç”¨ç»Ÿä¸€å·¥å…·å‡½æ•°è¯»å–"æ‘˜è¦"åˆ—
            abstracts = read_target_column_from_csv(selected_file, "æ‘˜è¦")
            if not abstracts:
                return
        
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—ï¼")
            return
    
    else:
        print("âŒ è¾“å…¥é”™è¯¯ï¼šè¯·é€‰æ‹©1æˆ–2ï¼")
        return
    
    # 3. åˆ†ç±»é¢„æµ‹
    print(f"\nğŸ” å¼€å§‹å¯¹{len(abstracts)}ç¯‡æ–‡çŒ®è¿›è¡Œåˆ†ç±»...")
    results = []
    
    for i, abstract in enumerate(abstracts, 1):
        if input_choice == "2":
            print(f"ğŸ“Š å¤„ç†è¿›åº¦ï¼š{i}/{len(abstracts)}", end="\r")
        
        # ç¼–ç æ–‡æœ¬
        encoded = tokenizer(
            abstract,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=256  # é€‚é…é•¿æ‘˜è¦
        ).to(device)
        
        # æå–ç‰¹å¾
        with torch.no_grad():
            outputs = model(**encoded)
            feat = outputs.pooler_output.to(torch.float16) ##

            # ä½¿ç”¨ç›¸ä¼¼åº¦åŒ¹é…
            pred_label_idx, pred_probs = predict_by_similarity(model, tokenizer, device, feat, csldcp_labels)
            pred_label = csldcp_labels[pred_label_idx]
            confidence = pred_probs[pred_label_idx]
        
        results.append({
            'abstract': abstract,
            'pred_label': pred_label,
            'confidence': confidence
        })
    
    # 4. è¾“å‡ºç»“æœ
    print("\n" + "="*40 + " åˆ†ç±»ç»“æœ " + "="*40)
    if input_choice == "1":
        # å•æ¡è¯¦ç»†ç»“æœ
        res = results[0]
        print(f"ğŸ“ æ‘˜è¦ï¼š{res['abstract'][:150]}..." if len(res['abstract']) > 150 else f"ğŸ“ æ‘˜è¦ï¼š{res['abstract']}")
        print(f"ğŸ·ï¸ é¢„æµ‹å­¦ç§‘ï¼š{res['pred_label']}")
        print(f"ğŸ“Š ç½®ä¿¡åº¦ï¼š{res['confidence']:.4f}")
    else:
        # æ‰¹é‡ç»Ÿè®¡+éƒ¨åˆ†è¯¦æƒ…
        print(f"ğŸ“ˆ æ‰¹é‡ç»Ÿè®¡ï¼šå…±{len(results)}ç¯‡æ–‡çŒ®")
        from collections import Counter
        label_counts = Counter([r['pred_label'] for r in results])
        print(f"\nğŸ“Š å­¦ç§‘åˆ†å¸ƒï¼ˆTop10ï¼‰ï¼š")
        for label, count in label_counts.most_common(10):  # æ˜¾ç¤ºTop10
            percentage = (count / len(results)) * 100
            print(f"  {label}ï¼š{count}ç¯‡ ({percentage:.1f}%)")
        
        # æ˜¾ç¤ºå‰5æ¡è¯¦æƒ…
        print(f"\nğŸ” å‰{min(5, len(results))}ç¯‡æ–‡çŒ®è¯¦æƒ…ï¼š")
        for i, res in enumerate(results[:5], 1):
            print(f"\n{i}. æ‘˜è¦ï¼š{res['abstract'][:100]}...")
            print(f"   å­¦ç§‘ï¼š{res['pred_label']} | ç½®ä¿¡åº¦ï¼š{res['confidence']:.4f}")
        if len(results) > 5:
            show_all = input(f"\nğŸ’¡ è¿˜æœ‰{len(results)-5}æ¡ç»“æœæœªæ˜¾ç¤ºï¼Œæ˜¯å¦æ˜¾ç¤ºå…¨éƒ¨ï¼Ÿ(y/n)ï¼š")
            if show_all.lower() == 'y':
                print("\n" + "="*20 + " å…¨éƒ¨ç»“æœè¯¦æƒ… " + "="*20)
                for i, res in enumerate(results, 1):
                    # æ‰“å°å®Œæ•´æ‘˜è¦ï¼ˆæˆ–é•¿ä¸€ç‚¹çš„æˆªæ–­ï¼‰
                    print(f"\n{i}. æ‘˜è¦ï¼š{res['abstract']}")
                    print(f"   å­¦ç§‘ï¼š{res['pred_label']} | ç½®ä¿¡åº¦ï¼š{res['confidence']:.4f}")


def iflytek_task(tokenizer, model, device):
    """ä»»åŠ¡6ï¼šåº”ç”¨ç®€ä»‹åˆ†ç±»"""
    global model_name
    print("\n" + "="*50)
    print(f"ğŸ“± ä»»åŠ¡6ï¼šåº”ç”¨ç®€ä»‹åˆ†ç±» - æ¨¡å‹: {model_name}")
    print("="*50)
    
    # 1. å®šä¹‰åº”ç”¨ç±»åˆ«
    app_labels = {
        0: "æ‰“è½¦", 1: "åœ°å›¾å¯¼èˆª", 2: "æ—…æ¸¸", 3: "å¤–å–", 4: "ç¾é£Ÿ", 5: "ç¤¾äº¤", 6: "è´­ç‰©",
        7: "è§†é¢‘", 8: "éŸ³ä¹", 9: "æ•™è‚²", 10: "åŠå…¬", 11: "å·¥å…·", 12: "é‡‘è", 13: "åŒ»ç–—å¥åº·",
        14: "å‡ºè¡Œ", 15: "æˆ¿äº§", 16: "æ‹›è˜", 17: "å°è¯´", 18: "èµ„è®¯", 19: "æ‘„å½±", 20: "ç¾å›¾",
        21: "æ¯å©´", 22: "è¿åŠ¨", 23: "ç¾å¦†", 24: "ä¸¤æ€§", 25: "åŠ¨æ¼«", 26: "æ¸¸æˆ", 27: "å¨±ä¹",
        28: "å½±è§†", 29: "æ˜Ÿåº§", 30: "ç›´æ’­", 31: "ç†è´¢", 32: "ä¿é™©", 33: "è´·æ¬¾", 34: "ä¿¡ç”¨å¡",
        35: "è¯åˆ¸", 36: "è‚¡ç¥¨", 37: "åŸºé‡‘", 38: "é“¶è¡Œ", 39: "æ”¯ä»˜", 40: "è®°è´¦", 41: "ç¨åŠ¡",
        42: "ç¤¾ä¿", 43: "åŒ»ä¿", 44: "åŒ»ç–—æœåŠ¡", 45: "å¥åº·ç®¡ç†", 46: "å°±åŒ»æŒ‚å·", 47: "è¯å“æŸ¥è¯¢",
        48: "ä½“æ£€", 49: "å…»ç”Ÿ", 50: "å‡è‚¥", 51: "è‚²å„¿", 52: "æ—©æ•™", 53: "K12æ•™è‚²", 54: "èŒä¸šæ•™è‚²",
        55: "è¯­è¨€å­¦ä¹ ", 56: "è€ƒç ”", 57: "å…¬è€ƒ", 58: "ç•™å­¦", 59: "æ±‚èŒ", 60: "èŒåœº", 61: "åŠå…¬åä½œ",
        62: "æ–‡æ¡£ç®¡ç†", 63: "ç¬”è®°", 64: "æ€ç»´å¯¼å›¾", 65: "PPT", 66: "è¡¨æ ¼", 67: "PDF", 68: "OCR",
        69: "ç¿»è¯‘", 70: "è¯å…¸", 71: "è®¡ç®—å™¨", 72: "æ—¥å†", 73: "å¤©æ°”", 74: "é—¹é’Ÿ", 75: "æ‰‹ç”µç­’",
        76: "æ–‡ä»¶ç®¡ç†", 77: "å‹ç¼©", 78: "åŠ å¯†", 79: "æ€æ¯’", 80: "æµè§ˆå™¨", 81: "è¾“å…¥æ³•", 82: "å£çº¸",
        83: "ä¸»é¢˜", 84: "é“ƒå£°", 85: "æ–‡ä»¶ä¼ è¾“", 86: "WiFi", 87: "è“ç‰™", 88: "æŠ•å±", 89: "è¿œç¨‹æ§åˆ¶",
        90: "æ™ºèƒ½å®¶å±…", 91: "æ±½è½¦æœåŠ¡", 92: "è¿ç« æŸ¥è¯¢", 93: "é©¾æ ¡", 94: "æ±½è½¦èµ„è®¯", 95: "äºŒæ‰‹è½¦",
        96: "ç§Ÿæˆ¿", 97: "ä¹°æˆ¿", 98: "è£…ä¿®", 99: "å®¶å±…å»ºæ", 100: "å®¶æ”¿", 101: "å¿«é€’", 102: "ç‰©æµ",
        103: "å¤–å–é…é€", 104: "é¤é¥®æœåŠ¡", 105: "é…’åº—é¢„è®¢", 106: "æœºç¥¨", 107: "ç«è½¦ç¥¨", 108: "ç§Ÿè½¦",
        109: "å…±äº«å•è½¦", 110: "å…¬äº¤", 111: "åœ°é“", 112: "è½®æ¸¡", 113: "é•¿é€”å®¢è¿", 114: "åœè½¦åœº",
        115: "åŠ æ²¹", 116: "æ´—è½¦", 117: "æ±½è½¦ç»´ä¿®", 118: "å…¶ä»–"
    }
    print(f"æ”¯æŒåˆ†ç±»ï¼š{', '.join([f'{k}:{v}' for k, v in app_labels.items()])}")
    
    # 2. è·å–ç”¨æˆ·è¾“å…¥æ–¹å¼
    print("\nè¯·é€‰æ‹©è¾“å…¥æ–¹å¼ï¼š")
    print("1. æ‰‹åŠ¨è¾“å…¥åº”ç”¨ç®€ä»‹")
    print("2. ä»Information/AppDescriptionsæ–‡ä»¶å¤¹è¯»å–CSVæ–‡ä»¶æ‰¹é‡åˆ†ç±»")
    input_choice = input("è¯·è¾“å…¥é€‰é¡¹ç¼–å·ï¼ˆ1/2ï¼‰ï¼š")
    
    descriptions = []
    if input_choice == "1":
        # æ‰‹åŠ¨è¾“å…¥
        user_desc = input("è¯·è¾“å…¥åº”ç”¨ç®€ä»‹æ–‡æœ¬ï¼š")
        if not user_desc.strip():
            print("âŒ è¾“å…¥é”™è¯¯ï¼šåº”ç”¨ç®€ä»‹ä¸èƒ½ä¸ºç©ºï¼")
            return
        descriptions = [user_desc]
    
    elif input_choice == "2":
        # æ‰¹é‡è¯»å–CSV
        script_dir = os.path.dirname(os.path.abspath(__file__))
        app_folder = os.path.join(script_dir, "Spiders", "Information", "AppDescriptions")
        if not os.path.exists(app_folder):
            print(f"âŒ AppDescriptionsæ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼š{app_folder}")
            print("ğŸ’¡ è¯·ç¡®ä¿Information/AppDescriptionsæ–‡ä»¶å¤¹ç»“æ„æ­£ç¡®")
            return
        
        csv_files = [f for f in os.listdir(app_folder) if f.endswith('.csv')]
        if not csv_files:
            print("âŒ AppDescriptionsæ–‡ä»¶å¤¹ä¸­æ— CSVæ–‡ä»¶")
            return
        
        # é€‰æ‹©CSVæ–‡ä»¶
        print(f"\nğŸ“ æ‰¾åˆ°{len(csv_files)}ä¸ªCSVæ–‡ä»¶ï¼š")
        for i, csv_file in enumerate(csv_files, 1):
            print(f"  {i}. {csv_file}")
        
        try:
            file_index = int(input(f"è¯·é€‰æ‹©æ–‡ä»¶ç¼–å·ï¼ˆ1-{len(csv_files)}ï¼‰ï¼š")) - 1
            if file_index < 0 or file_index >= len(csv_files):
                print("âŒ ç¼–å·è¶…å‡ºèŒƒå›´ï¼")
                return
            selected_file = os.path.join(app_folder, csv_files[file_index])
            
            # ä½¿ç”¨ç»Ÿä¸€å·¥å…·å‡½æ•°è¯»å–"åº”ç”¨ç®€ä»‹"åˆ—
            descriptions = read_target_column_from_csv(selected_file, "åº”ç”¨ç®€ä»‹")
            if not descriptions:
                return
        
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—ï¼")
            return
    
    else:
        print("âŒ è¾“å…¥é”™è¯¯ï¼šè¯·é€‰æ‹©1æˆ–2ï¼")
        return
    
    # 3. åˆ†ç±»é¢„æµ‹
    print(f"\nğŸ” å¼€å§‹å¯¹{len(descriptions)}æ¡åº”ç”¨ç®€ä»‹è¿›è¡Œåˆ†ç±»...")
    results = []
    
    for i, desc in enumerate(descriptions, 1):
        if input_choice == "2":
            print(f"ğŸ“Š å¤„ç†è¿›åº¦ï¼š{i}/{len(descriptions)}", end="\r")
        
        # ç¼–ç æ–‡æœ¬
        encoded = tokenizer(
            desc,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=128
        ).to(device)
        
        # æå–ç‰¹å¾
        with torch.no_grad():
            outputs = model(**encoded)
            feat = outputs.pooler_output.to(torch.float16) ##

            # ä½¿ç”¨ç›¸ä¼¼åº¦åŒ¹é…
            pred_label_idx, pred_probs = predict_by_similarity(model, tokenizer, device, feat, app_labels)
            pred_label = app_labels[pred_label_idx]
            confidence = pred_probs[pred_label_idx]
        
        results.append({
            'description': desc,
            'pred_label': pred_label,
            'confidence': confidence
        })
    
    # 4. è¾“å‡ºç»“æœ
    print("\n" + "="*40 + " åˆ†ç±»ç»“æœ " + "="*40)
    if input_choice == "1":
        # å•æ¡è¯¦ç»†ç»“æœ
        res = results[0]
        print(f"ğŸ“ åº”ç”¨ç®€ä»‹ï¼š{res['description'][:150]}..." if len(res['description']) > 150 else f"ğŸ“ åº”ç”¨ç®€ä»‹ï¼š{res['description']}")
        print(f"ğŸ·ï¸ é¢„æµ‹ç±»åˆ«ï¼š{res['pred_label']}")
        print(f"ğŸ“Š ç½®ä¿¡åº¦ï¼š{res['confidence']:.4f}")
    else:
        # æ‰¹é‡ç»Ÿè®¡+éƒ¨åˆ†è¯¦æƒ…
        print(f"ğŸ“ˆ æ‰¹é‡ç»Ÿè®¡ï¼šå…±{len(results)}æ¡åº”ç”¨ç®€ä»‹")
        from collections import Counter
        label_counts = Counter([r['pred_label'] for r in results])
        print(f"\nğŸ“Š ç±»åˆ«åˆ†å¸ƒï¼š")
        for label in app_labels.values():
            count = label_counts.get(label, 0)
            percentage = (count / len(results)) * 100
            print(f"  {label}ï¼š{count}æ¡ ({percentage:.1f}%)")
        
        # æ˜¾ç¤ºå‰5æ¡è¯¦æƒ…
        print(f"\nğŸ” å‰{min(5, len(results))}æ¡åº”ç”¨ç®€ä»‹è¯¦æƒ…ï¼š")
        for i, res in enumerate(results[:5], 1):
            print(f"\n{i}. ç®€ä»‹ï¼š{res['description'][:100]}...")
            print(f"   ç±»åˆ«ï¼š{res['pred_label']} | ç½®ä¿¡åº¦ï¼š{res['confidence']:.4f}")


def cluewsc_task(tokenizer, model, device): # ç§»é™¤configå‚æ•°ï¼Œç›´æ¥ä½¿ç”¨å…¨å±€config
    """ä»»åŠ¡7ï¼šCLUEWSCï¼ˆæŒ‡ä»£æ¶ˆè§£ï¼‰"""
    global model_name, config
    print("\n" + "="*50)
    print(f"ğŸ” ä»»åŠ¡7ï¼šæŒ‡ä»£æ¶ˆè§£ï¼ˆCLUEWSCï¼‰ - æ¨¡å‹: {model_name}")
    print("="*50)
    
    # 1. ä»»åŠ¡è¯´æ˜
    print("ä»»åŠ¡ç›®æ ‡ï¼šåˆ¤æ–­å¥å­ä¸­ä»£è¯æ˜¯å¦ä¸æŒ‡å®šåè¯çŸ­è¯­å…±æŒ‡ï¼ˆæŒ‡ä»£åŒä¸€å¯¹è±¡ï¼‰")
    print("ç¤ºä¾‹ï¼šå¥å­='å°æ˜å‘Šè¯‰å°åä»–è€ƒè¯•ä¸åŠæ ¼'ï¼Œåè¯çŸ­è¯­='å°æ˜' â†’ å…±æŒ‡ï¼ˆTrueï¼‰/ä¸å…±æŒ‡ï¼ˆFalseï¼‰")
    
    # 2. è·å–ç”¨æˆ·è¾“å…¥
    user_sentence = input("è¯·è¾“å…¥åŒ…å«ä»£è¯çš„å¥å­ï¼š")
    noun_phrase = input("è¯·è¾“å…¥éœ€è¦åˆ¤æ–­çš„åè¯çŸ­è¯­ï¼ˆå¦‚ï¼šå°æ˜ã€è¿™æœ¬ä¹¦ï¼‰ï¼š")
    
    if not user_sentence.strip() or not noun_phrase.strip():
        print("âŒ è¾“å…¥é”™è¯¯ï¼šå¥å­å’Œåè¯çŸ­è¯­ä¸èƒ½ä¸ºç©ºï¼")
        return
    
    # 3. ç¼–ç æ–‡æœ¬ï¼ˆç”¨[SEP]åˆ†éš”å¥å­å’Œåè¯çŸ­è¯­ï¼‰
    wsc_text = f"{user_sentence} {tokenizer.sep_token} {noun_phrase}"
    encoded = tokenizer(
        wsc_text,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=128
    ).to(device)
    
    # 4. æå–ç‰¹å¾å¹¶é¢„æµ‹
    with torch.no_grad():
        outputs = model(**encoded)
        feat = outputs.pooler_output.to(torch.float16) ##
    
    # äºŒåˆ†ç±»å¤´ï¼ˆå…±æŒ‡/ä¸å…±æŒ‡ï¼‰
    input_size = config.hidden_size
    # æ³¨æ„ï¼šè¿™ä¸ªåˆ†ç±»å¤´æ˜¯éšæœºåˆå§‹åŒ–çš„ï¼Œå¯¹äºé›¶æ ·æœ¬æ•ˆæœå¯èƒ½ä¸ä½³ï¼Œä½†ä¿æŒåŸé€»è¾‘
    wsc_classifier = torch.nn.Linear(input_size, 2).to(device, dtype=torch.float16)
    with torch.no_grad():
        logits = wsc_classifier(feat)
        pred_probs = F.softmax(logits, dim=1).cpu().numpy()[0]
        pred_label_idx = torch.argmax(logits).item()
        is_coreferent = pred_label_idx == 0  # 0=å…±æŒ‡ï¼Œ1=ä¸å…±æŒ‡
        confidence = pred_probs[pred_label_idx]
    
    # 5. è¾“å‡ºç»“æœ
    print("\n" + "="*30 + " ç»“æœ " + "="*30)
    print(f"åŸå¥å­ï¼š{user_sentence}")
    print(f"åè¯çŸ­è¯­ï¼š{noun_phrase}")
    print(f"å…±æŒ‡å…³ç³»ï¼š{'âœ… æ˜¯' if is_coreferent else 'âŒ å¦'}")
    print(f"ç½®ä¿¡åº¦ï¼š{confidence:.4f}")
    print(f"è¯¦ç»†æ¦‚ç‡ï¼šå…±æŒ‡æ¦‚ç‡ {pred_probs[0]:.4f} | ä¸å…±æŒ‡æ¦‚ç‡ {pred_probs[1]:.4f}")


# ä¸»å‡½æ•°ï¼ˆç”¨äºæµ‹è¯•å’Œè¿è¡Œï¼‰
if __name__ == "__main__":
    
    # è·å–è„šæœ¬æ‰€åœ¨çš„æ ¹ç›®å½• (Program ç›®å½•)
    script_dir = os.path.dirname(os.path.abspath(__file__))

    # 1. é¡¶å±‚é€‰æ‹©é€»è¾‘
    while True:
        print("\n" + "="*50)
        print("æ¬¢è¿ä½¿ç”¨Erlangshen-MegatronBert æ¨¡å‹å’Œçˆ¬è™«å·¥å…·")
        print("="*50)
        print("\nè¯·é€‰æ‹©åŠŸèƒ½ï¼š")
        print("1. è¿è¡Œçˆ¬è™«å¹¶çˆ¬å–ç½‘é¡µ")
        print("2. åŠ è½½æ¨¡å‹å¹¶æ‰§è¡Œä»»åŠ¡")
        print("0. é€€å‡º")
        
        main_choice = input("è¯·è¾“å…¥é€‰é¡¹ç¼–å·ï¼ˆ1/2/0ï¼‰ï¼š").strip()
        
        if main_choice == "1":
            # è¿è¡Œçˆ¬è™«è„šæœ¬
            spider_launcher_path = os.path.join(script_dir, "Spiders", "spider_launcher.py")
            if os.path.exists(spider_launcher_path):
                print(f"\nğŸ”„ æ­£åœ¨è¿è¡Œçˆ¬è™«è„šæœ¬: {spider_launcher_path}")
                # ä½¿ç”¨ os.system æˆ– subprocess è¿è¡Œå¦ä¸€ä¸ª Python è„šæœ¬
                # è¿™é‡Œä½¿ç”¨ os.system ç®€åŒ–æ“ä½œï¼Œå¦‚æœéœ€è¦æ›´å¤æ‚çš„æ§åˆ¶ï¼Œåº”ä½¿ç”¨ subprocess
                try:
                    os.system(f"{sys.executable} {spider_launcher_path}")
                except Exception as e:
                    print(f"âŒ è¿è¡Œçˆ¬è™«è„šæœ¬å¤±è´¥ï¼š{e}")
            else:
                print(f"âŒ çˆ¬è™«è„šæœ¬ä¸å­˜åœ¨ï¼š{spider_launcher_path}")
            
        elif main_choice == "2":
            # è¿›å…¥æ¨¡å‹é€‰æ‹©å’Œä»»åŠ¡æ‰§è¡Œæµç¨‹
            
            # æ¨¡å‹é€‰æ‹©é€»è¾‘
            MODEL_MAP = {
                "1": "Erlangshen-MegatronBert-1.3B",
                "2": "Erlangshen-MegatronBert-3.9B",
            }
            
            while True:
                print("\nè¯·é€‰æ‹©éœ€è¦ä½¿ç”¨çš„æ¨¡å‹ï¼š")
                for key, name in MODEL_MAP.items():
                    print(f"{key}. {name}")
                
                model_choice = input("è¯·è¾“å…¥æ¨¡å‹ç¼–å·ï¼ˆ1/2ï¼‰ï¼š").strip()
                
                if model_choice in MODEL_MAP:
                    selected_model_name = MODEL_MAP[model_choice]
                    
                    # æ‹¼æ¥å‡ºé€‰ä¸­æ¨¡å‹çš„å®Œæ•´è·¯å¾„ï¼š Models/Erlangshen-MegatronBert-X.XB
                    model_dir = os.path.join(script_dir, "Models", selected_model_name)
                    
                    # æ£€æŸ¥ Models æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
                    if not os.path.exists(os.path.join(script_dir, "Models")):
                        print(f"âŒ æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼š{os.path.join(script_dir, 'Models')}")
                        print("è¯·å°†æ¨¡å‹æ–‡ä»¶å¤¹æ”¾å…¥åä¸º 'Models' çš„æ–°æ–‡ä»¶å¤¹ä¸­ï¼")
                        sys.exit(1)

                    init_model_and_tokenizer(model_dir, selected_model_name)
                    
                    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æˆåŠŸåŠ è½½
                    if model is not None:
                        break # æ¨¡å‹åŠ è½½æˆåŠŸï¼Œé€€å‡ºæ¨¡å‹é€‰æ‹©å¾ªç¯
                    else:
                        print("æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œå†…å®¹ã€‚")
                        continue # è¿”å›æ¨¡å‹é€‰æ‹©
                        
                else:
                    print("æ— æ•ˆçš„æ¨¡å‹ç¼–å·ï¼Œè¯·é‡è¯•ï¼")
                    continue
            
            # æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè¿›å…¥ä»»åŠ¡é€‰æ‹©é€»è¾‘
            while True:
                print("\nè¯·é€‰æ‹©ä»»åŠ¡ï¼š")
                print("1. æˆè¯­å¡«ç©ºï¼ˆCHIDFï¼‰")
                print("2. æ–°é—»åˆ†ç±»ï¼ˆTNEWSï¼‰")
                print("3. è‡ªç„¶è¯­è¨€æ¨ç†ï¼ˆOCNLIï¼‰")
                print("4. æ‘˜è¦å…³é”®è¯éªŒè¯ï¼ˆCSLï¼‰")
                print("5. ä¸»é¢˜æ–‡çŒ®åˆ†ç±»ï¼ˆCSLDCPï¼‰")
                print("6. åº”ç”¨æè¿°åˆ†ç±»ï¼ˆIFLYTEKï¼‰")
                print("7. æŒ‡ä»£æ¶ˆè§£ï¼ˆCLUEWSCï¼‰")
                print("0. è¿”å›ä¸»èœå•")
                
                choice = input("è¯·è¾“å…¥ä»»åŠ¡ç¼–å·ï¼š")
                
                if choice == "1":
                    chidf_task(tokenizer, model, device)
                elif choice == "2":
                    tnews_task(tokenizer, model, device)
                elif choice == "3":
                    ocnli_task(tokenizer, model, device)
                elif choice == "4":
                    csl_task(tokenizer, model, device)
                elif choice == "5":
                    csldcp_task(tokenizer, model, device)
                elif choice == "6":
                    iflytek_task(tokenizer, model, device)
                elif choice == "7":
                    cluewsc_task(tokenizer, model, device)
                elif choice == "0":
                    print("è¿”å›ä¸»èœå•...")
                    break  # é€€å‡ºä»»åŠ¡å¾ªç¯ï¼Œè¿”å›é¡¶å±‚ä¸»èœå•
                else:
                    print("æ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·é‡è¯•")
                    
        elif main_choice == "0":
            print("æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
            sys.exit(0) # é€€å‡ºæ•´ä¸ªç¨‹åº
        
        else:
            print("æ— æ•ˆçš„é€‰é¡¹ï¼Œè¯·é‡æ–°è¾“å…¥ï¼")