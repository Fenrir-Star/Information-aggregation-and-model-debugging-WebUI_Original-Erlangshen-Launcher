import requests
from datetime import datetime, timedelta
from lxml import etree
import csv
import os
import time
import random
import sys
import io


# è®¾ç½®æ ‡å‡†è¾“å‡ºç¼–ç 
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')


def get_news_folder():
    """è·å–æ–°é—»æ•°æ®ä¿å­˜ç›®å½•
    
    è‹¥ç›®å½•ä¸å­˜åœ¨åˆ™è‡ªåŠ¨åˆ›å»ºï¼Œç¡®ä¿æ•°æ®æœ‰æ­£ç¡®çš„ä¿å­˜ä½ç½®ã€‚
    
    Returns:
        str: æ–°é—»ä¿å­˜ç›®å½•è·¯å¾„
    """
    # è·å–å½“å‰è„šæœ¬è·¯å¾„
    script_path = os.path.abspath(__file__)
    script_dir = os.path.dirname(script_path)
    base_dir = os.path.dirname(script_dir)
    news_folder = os.path.join(base_dir, "Information", "News")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    if not os.path.exists(news_folder):
        os.makedirs(news_folder)
        print(f"âœ… å·²åˆ›å»ºNewsæ–‡ä»¶å¤¹: {news_folder}")
    
    return news_folder


def parse_time(unformated_time):
    """è§£æç›¸å¯¹æ—¶é—´ä¸ºæ ‡å‡†æ ¼å¼
    
    å°†"Xåˆ†é’Ÿå‰"ã€"Xå°æ—¶å‰"ç­‰ç›¸å¯¹æ—¶é—´è½¬æ¢ä¸ºç»å¯¹æ—¶é—´å­—ç¬¦ä¸²ã€‚
    
    Args:
        unformated_time (str): åŸå§‹æ—¶é—´å­—ç¬¦ä¸²
        
    Returns:
        str: æ ¼å¼åŒ–åçš„æ—¶é—´å­—ç¬¦ä¸²ï¼ˆYYYY-MM-DD HH:MMï¼‰
    """
    if 'åˆ†é’Ÿ' in unformated_time:
        minute = unformated_time[:unformated_time.find('åˆ†é’Ÿ')]
        minute = timedelta(minutes=int(minute))
        return (datetime.now() - minute).strftime('%Y-%m-%d %H:%M')
    elif 'å°æ—¶' in unformated_time:
        hour = unformated_time[:unformated_time.find('å°æ—¶')]
        hour = timedelta(hours=int(hour))
        return (datetime.now() - hour).strftime('%Y-%m-%d %H:%M')
    else:
        return unformated_time


def deal_html(html, file_name):
    """å¤„ç†HTMLé¡µé¢ï¼Œæå–æ–°é—»ä¿¡æ¯å¹¶ä¿å­˜
    
    Args:
        html (lxml.etree._Element): è§£æåçš„HTMLå¯¹è±¡
        file_name (str): ä¿å­˜æ–‡ä»¶è·¯å¾„
    """
    # å°è¯•å¤šç§é€‰æ‹©å™¨åŒ¹é…æ–°é—»ç»“æœï¼ˆåº”å¯¹é¡µé¢ç»“æ„å˜åŒ–ï¼‰
    results = []
    results = html.xpath('//div[contains(@class, "result-op") and contains(@class, "c-container")]')
    
    if not results:
        results = html.xpath('//div[@class="result-op c-container xpath-log new-pmd"]')
    
    if not results:
        results = html.xpath('//div[contains(@class, "news-result")]')
    
    if not results:
        results = html.xpath('//div[@class="result"]')
    
    print(f"ğŸ“Š æ‰¾åˆ° {len(results)} æ¡æ–°é—»ç»“æœ")
    
    save_data = []

    for i, result in enumerate(results):
        try:
            # æå–æ ‡é¢˜
            title_elements = (result.xpath('.//h3/a') or 
                             result.xpath('.//h3') or 
                             result.xpath('.//a[@class="news-title"]'))
            if not title_elements:
                continue
                
            title = title_elements[0].xpath('string(.)').strip()

            # æå–æ‘˜è¦
            summary_elements = (result.xpath('.//span[@class="c-font-normal c-color-text"]') or 
                              result.xpath('.//div[contains(@class, "c-span-last")]') or 
                              result.xpath('.//div[contains(@class, "c-gap-top-xsmall")]'))
            summary = summary_elements[0].xpath('string(.)').strip() if summary_elements else ""

            # æå–æ¥æºå’Œæ—¶é—´
            source = ""
            date_time = ""
            
            # å°è¯•å¤šç§ä¿¡æ¯é€‰æ‹©å™¨
            info_elements = (result.xpath('.//div[contains(@class, "news-source")]') or 
                           result.xpath('.//span[contains(@class, "c-color-gray")]') or 
                           result.xpath('.//div[contains(@class, "c-span-last")]//span'))
            
            if info_elements:
                info_text = info_elements[0].xpath('string(.)').strip()
                # åˆ†ç¦»æ¥æºå’Œæ—¶é—´
                if 'Â·' in info_text:
                    parts = info_text.split('Â·')
                    if len(parts) >= 2:
                        source = parts[0].strip()
                        date_time = parse_time(parts[1].strip())
                else:
                    source = info_text
                    date_time = "æœªçŸ¥æ—¶é—´"
            
            # å•ç‹¬æå–æ—¶é—´ï¼ˆå…œåº•ï¼‰
            if not date_time:
                time_elements = result.xpath('.//span[@class="c-color-gray2 c-font-normal c-gap-right-xsmall"]/text()')
                if time_elements:
                    date_time = parse_time(time_elements[0])
                else:
                    date_time = "æœªçŸ¥æ—¶é—´"

            print(f'ç¬¬{i+1}æ¡æ–°é—»:')
            print(f'æ ‡é¢˜: {title}')
            print(f'æ¥æº: {source}')
            print(f'æ—¶é—´: {date_time}')
            print(f'æ¦‚è¦: {summary}')
            print('-' * 50)

            save_data.append({
                'title': title,
                'source': source,
                'time': date_time,
                'summary': summary
            })
            
        except Exception as e:
            print(f"âŒ è§£æç¬¬{i+1}æ¡æ–°é—»æ—¶å‡ºé”™: {e}")
            continue
    
    # å†™å…¥CSVæ–‡ä»¶
    if save_data:
        file_exists = os.path.exists(file_name)
        with open(file_name, 'a+', encoding='utf-8-sig', newline='') as f:
            writer = csv.writer(f)
            # å†™å…¥è¡¨å¤´ï¼ˆä»…é¦–æ¬¡ï¼‰
            if not file_exists or os.path.getsize(file_name) == 0:
                writer.writerow(['æ ‡é¢˜', 'æ¥æº', 'æ—¶é—´', 'æ¦‚è¦'])
            for row in save_data:
                writer.writerow([row['title'], row['source'], row['time'], row['summary']])
        print(f"âœ… æˆåŠŸä¿å­˜ {len(save_data)} æ¡æ–°é—»åˆ°æ–‡ä»¶")
    else:
        print("âŒ æ²¡æœ‰æå–åˆ°ä»»ä½•æ–°é—»æ•°æ®")


# è¯·æ±‚é…ç½®
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Referer': 'https://www.baidu.com/',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Cache-Control': 'no-cache',
    'Pragma': 'no-cache'
}

URL = 'https://www.baidu.com/s'

PARAMS = {
    'ie': 'utf-8',
    'medium': 0,
    # rtt=4 æŒ‰æ—¶é—´æ’åºï¼›rtt=1 æŒ‰ç„¦ç‚¹æ’åº
    'rtt': 1,
    'bsst': 1,
    'rsv_dl': 'news_t_sk',
    'cl': 2,
    'tn': 'news',
    'rsv_bp': 1,
    'oq': '',
    'rsv_btype': 't',
    'f': 8,
    'wd': ''  # æœç´¢å…³é”®è¯ï¼ˆåç»­å¡«å……ï¼‰
}


def do_spider(keyword, sort_by='focus'):
    """æ‰§è¡Œç™¾åº¦æ–°é—»çˆ¬è™«
    
    Args:
        keyword (str): æœç´¢å…³é”®è¯
        sort_by (str): æ’åºæ–¹å¼ï¼Œ'focus'ï¼ˆæŒ‰ç„¦ç‚¹ï¼‰æˆ–'time'ï¼ˆæŒ‰æ—¶é—´ï¼‰ï¼Œé»˜è®¤'focus'
    """
    # è·å–ä¿å­˜ç›®å½•
    news_folder = get_news_folder()
    file_name = os.path.join(news_folder, f'{keyword}.csv')

    print(f"ğŸ“ æ–‡ä»¶å°†ä¿å­˜åˆ°: {file_name}")

    # é…ç½®è¯·æ±‚å‚æ•°
    PARAMS['wd'] = keyword
    PARAMS['rtt'] = 4 if sort_by == 'time' else 1

    try:
        print(f"ğŸ” å¼€å§‹è¯·æ±‚ç™¾åº¦æ–°é—»ï¼Œå…³é”®è¯: {keyword}")
        response = requests.get(url=URL, params=PARAMS, headers=HEADERS, timeout=10)
        response.encoding = 'utf-8'
        
        # ä¿å­˜ç½‘é¡µç”¨äºè°ƒè¯•
        debug_html_path = os.path.join(news_folder, 'debug_baidu_news.html')
        with open(debug_html_path, 'w', encoding='utf-8') as f:
            f.write(response.text)
        print(f"ğŸ’¾ ç½‘é¡µå·²ä¿å­˜åˆ°: {debug_html_path}")
        
        html = etree.HTML(response.text)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æœ
        no_results = html.xpath('//div[contains(text(), "æ²¡æœ‰æ‰¾åˆ°")]') or html.xpath('//div[contains(text(), "æœªæ‰¾åˆ°")]')
        if no_results:
            print("âŒ ç™¾åº¦è¿”å›ï¼šæ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–°é—»")
            return
        
        deal_html(html, file_name)

        # å°è¯•è·å–æ€»é¡µæ•°å¹¶çˆ¬å–å¤šé¡µ
        try:
            total_element = (html.xpath('//div[@id="header_top_bar"]/span/text()') or 
                           html.xpath('//span[@class="nums_text"]/text()') or 
                           html.xpath('//div[contains(@class, "nums")]//text()'))
            
            if total_element:
                total_text = total_element[0] if total_element else ""
                print(f"æ€»ç»“æœä¿¡æ¯: {total_text}")
                
                # æå–æ€»æ¡æ•°
                import re
                numbers = re.findall(r'\d+', total_text.replace(',', ''))
                if numbers:
                    total = int(numbers[0])
                    page_num = min(total // 10, 5)  # é™åˆ¶æœ€å¤š5é¡µ
                
                    print(f"ğŸ“„ æ€»å…±çº¦ {total} æ¡ç»“æœï¼Œè®¡åˆ’çˆ¬å– {page_num} é¡µ")
                    
                    for page in range(1, page_num + 1):
                        print(f'\nç¬¬ {page} é¡µ\n')
                        HEADERS['Referer'] = response.url
                        PARAMS['pn'] = page * 10

                        response = requests.get(url=URL, headers=HEADERS, params=PARAMS, timeout=10)
                        response.encoding = 'utf-8'
                        
                        html = etree.HTML(response.text)
                        deal_html(html, file_name)

                        time.sleep(random.randint(2, 4))
                else:
                    print("æ— æ³•è§£ææ€»é¡µæ•°ï¼Œåªçˆ¬å–ç¬¬ä¸€é¡µ")
            else:
                print("æœªæ‰¾åˆ°æ€»é¡µæ•°ä¿¡æ¯ï¼Œåªçˆ¬å–ç¬¬ä¸€é¡µ")
                
        except Exception as e:
            print(f"âŒ åˆ†é¡µçˆ¬å–å‡ºé”™: {e}ï¼Œç»§ç»­å¤„ç†ç¬¬ä¸€é¡µæ•°æ®")
            
    except requests.RequestException as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚å‡ºé”™: {e}")
    except Exception as e:
        print(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


def main():
    """ç™¾åº¦æ–°é—»çˆ¬è™«ä¸»å…¥å£"""
    try:
        # ç”¨æˆ·è¾“å…¥é…ç½®
        keyword = input("è¯·è¾“å…¥æœç´¢å…³é”®è¯ï¼ˆé»˜è®¤ï¼šç‰¹æœ—æ™®ï¼‰: ").strip()
        if not keyword:
            keyword = 'ç‰¹æœ—æ™®'
        
        sort_option = input("è¯·é€‰æ‹©æ’åºæ–¹å¼ï¼š1-æŒ‰ç„¦ç‚¹æ’åºï¼Œ2-æŒ‰æ—¶é—´æ’åºï¼ˆé»˜è®¤ï¼š1ï¼‰: ").strip()
        sort_by = 'focus' if sort_option != '2' else 'time'
        
        print(f"ğŸ” å¼€å§‹çˆ¬å–å…³é”®è¯: {keyword}")
        print(f"ğŸ“Š æ’åºæ–¹å¼: {'æŒ‰ç„¦ç‚¹æ’åº' if sort_by == 'focus' else 'æŒ‰æ—¶é—´æ’åº'}")
        
        do_spider(keyword=keyword, sort_by=sort_by)
        
        # æ˜¾ç¤ºä¿å­˜ç»“æœ
        news_folder = get_news_folder()
        file_path = os.path.join(news_folder, f"{keyword}.csv")
        print(f"âœ… ç™¾åº¦æ–°é—»çˆ¬å–å®Œæˆï¼")
        print(f"ğŸ’¾ æ–‡ä»¶å·²ä¿å­˜è‡³: {file_path}")
        
        # ç®€å•éªŒè¯æ–‡ä»¶å†…å®¹
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8-sig') as f:
                content = f.read()
                lines = content.strip().split('\n')
                print(f"ğŸ“Š æ–‡ä»¶åŒ…å« {len(lines)} è¡Œæ•°æ®")
                if lines:
                    print("å‰å‡ è¡Œå†…å®¹:")
                    for i, line in enumerate(lines[:3]):
                        print(f"{i+1}: {line}")
    except Exception as e:
        print(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


def run():
    """è¿è¡Œå…¥å£ï¼ˆå…¼å®¹å¤–éƒ¨è°ƒç”¨ï¼‰"""
    main()


if __name__ == "__main__":
    main()