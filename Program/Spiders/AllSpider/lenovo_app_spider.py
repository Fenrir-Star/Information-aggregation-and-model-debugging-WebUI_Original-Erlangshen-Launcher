import os
import time
import csv
import random
from DrissionPage import ChromiumPage, ChromiumOptions


class LenovoSpider:
    """è”æƒ³åº”ç”¨å•†åº—çˆ¬è™«ç±»
    
    ç”¨äºçˆ¬å–è”æƒ³åº”ç”¨å•†åº—ä¸­æŒ‡å®šå…³é”®è¯çš„åº”ç”¨ä¿¡æ¯ï¼ŒåŒ…æ‹¬åº”ç”¨åç§°ã€ç®€ä»‹å’Œè¯¦æƒ…é“¾æ¥ï¼Œ
    å¹¶å°†ç»“æœä¿å­˜åˆ°CSVæ–‡ä»¶ä¸­ã€‚
    """
    
    def __init__(self):
        """åˆå§‹åŒ–çˆ¬è™«å®ä¾‹ï¼Œé…ç½®æµè§ˆå™¨é€‰é¡¹å¹¶åˆ›å»ºé¡µé¢å¯¹è±¡"""
        co = ChromiumOptions()

        # é…ç½®æµè§ˆå™¨è·¯å¾„
        browser_path = r"C:\Program Files (x86)\Microsoft\Edge\Application\msedge.exe"

        if os.path.exists(browser_path):
            co.set_paths(browser_path=browser_path)
        else:
            print(f"âš ï¸ è­¦å‘Š: è·¯å¾„ {browser_path} ä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤é…ç½®...")

        # æµè§ˆå™¨å‚æ•°é…ç½®
        co.set_argument('--disable-blink-features=AutomationControlled')
        co.set_argument('--no-sandbox')

        self.page = ChromiumPage(co)
        self.app_folder = self.init_folder()

    def init_folder(self):
        """åˆå§‹åŒ–åº”ç”¨æ•°æ®ä¿å­˜ç›®å½•
        
        å°è¯•åˆ›å»ºæŒ‡å®šçš„ä¿å­˜ç›®å½•ï¼Œè‹¥å¤±è´¥åˆ™å›é€€åˆ°è„šæœ¬æ‰€åœ¨ç›®å½•åˆ›å»ºé»˜è®¤æ–‡ä»¶å¤¹ã€‚
        
        Returns:
            str: å®é™…çš„ä¿å­˜ç›®å½•è·¯å¾„
        """
        script_path = os.path.abspath(__file__)
        script_dir = os.path.dirname(script_path)
        base_dir = os.path.dirname(script_dir)
        folder = os.path.join(base_dir, "Information", "AppDescriptions")

        # å°è¯•åˆ›å»ºç›®å½•
        if not os.path.exists(folder):
            try:
                os.makedirs(folder, exist_ok=True)
                print(f"âœ… å·²åˆ›å»ºç›®å½•: {folder}")
            except Exception as e:
                print(f"âŒ åˆ›å»ºç›®å½•å¤±è´¥: {e}")
                # å›é€€åˆ°è„šæœ¬æ‰€åœ¨ç›®å½•
                script_dir = os.path.dirname(os.path.abspath(__file__))
                folder = os.path.join(script_dir, "Lenovo_Apps")
                os.makedirs(folder, exist_ok=True)
                print(f"âš ï¸ å·²å›é€€åˆ°é»˜è®¤ç›®å½•: {folder}")

        print(f"\nğŸ“‚ æ•°æ®ä¿å­˜ç›®å½•: {folder}")
        return folder

    def save_to_csv(self, data, filename):
        """å°†åº”ç”¨ä¿¡æ¯ä¿å­˜åˆ°CSVæ–‡ä»¶
        
        Args:
            data (dict): åŒ…å«åº”ç”¨ä¿¡æ¯çš„å­—å…¸ï¼Œéœ€åŒ…å«'name'ã€'desc'ã€'url'é”®
            filename (str): ä¿å­˜çš„æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        """
        filepath = os.path.join(self.app_folder, f"{filename}.csv")
        file_exists = os.path.exists(filepath)

        try:
            with open(filepath, 'a', encoding='utf-8-sig', newline='') as f:
                writer = csv.writer(f)
                # å†™å…¥è¡¨å¤´ï¼ˆä»…é¦–æ¬¡ï¼‰
                if not file_exists:
                    writer.writerow(['åº”ç”¨åç§°', 'åº”ç”¨ç®€ä»‹', 'è¯¦æƒ…é“¾æ¥'])

                writer.writerow([
                    data.get('name', 'æœªçŸ¥'),
                    data.get('desc', ''),
                    data.get('url', '')
                ])
            print(f"âœ… æˆåŠŸä¿å­˜: {data.get('name')}")
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")

    def crawl_lenovo(self, keyword, max_apps=5):
        """çˆ¬å–è”æƒ³åº”ç”¨å•†åº—ä¸­æŒ‡å®šå…³é”®è¯çš„åº”ç”¨
        
        Args:
            keyword (str): æœç´¢å…³é”®è¯
            max_apps (int): æœ€å¤§çˆ¬å–åº”ç”¨æ•°é‡ï¼Œé»˜è®¤5
        """
        print(f"ğŸš€ å¼€å§‹æœç´¢: {keyword}")

        search_url = f"https://lestore.lenovo.com/search?k={keyword}"
        self.page.get(search_url)
        time.sleep(3)

        # æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå†…å®¹
        self.page.scroll.to_bottom()
        time.sleep(1)

        print("ğŸ” æå–åº”ç”¨é“¾æ¥...")
        link_elements = self.page.eles('xpath://a[contains(@href, "/detail/")]')

        # å»é‡å¤„ç†åº”ç”¨é“¾æ¥
        unique_urls = set()
        for ele in link_elements:
            if ele.link:
                unique_urls.add(ele.link)

        target_urls = list(unique_urls)[:max_apps]

        if not target_urls:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•åº”ç”¨é“¾æ¥ã€‚")
            return

        print(f"ğŸ“Š æ‰¾åˆ° {len(target_urls)} ä¸ªåº”ç”¨ï¼Œå¼€å§‹æŠ“å–...")

        # é€ä¸ªå¤„ç†åº”ç”¨è¯¦æƒ…é¡µ
        count = 0
        for url in target_urls:
            count += 1
            print(f"\n[{count}/{len(target_urls)}] æ­£åœ¨å¤„ç†: {url}")
            self.parse_detail_page(url, keyword)
            time.sleep(1)

    def parse_detail_page(self, url, keyword):
        """è§£æåº”ç”¨è¯¦æƒ…é¡µï¼Œæå–åº”ç”¨ä¿¡æ¯
        
        Args:
            url (str): åº”ç”¨è¯¦æƒ…é¡µé“¾æ¥
            keyword (str): æœç´¢å…³é”®è¯ï¼ˆç”¨äºä¿å­˜æ–‡ä»¶ï¼‰
        """
        tab = None
        try:
            tab = self.page.new_tab(url)

            # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
            start_time = time.time()
            while time.time() - start_time < 10:
                if tab.title and "è”æƒ³" in tab.title:
                    break
                time.sleep(0.5)

            current_title = tab.title
            if not current_title or "404" in current_title:
                print(f"âš ï¸ é¡µé¢æ— æ•ˆï¼Œè·³è¿‡: {url}")
                return

            # æå–åº”ç”¨åç§°
            name = "æœªçŸ¥åº”ç”¨"
            # ç­–ç•¥A: å°è¯•ä»h1æ ‡ç­¾æå–
            try:
                h1 = tab.ele('tag:h1', timeout=2)
                if h1: 
                    name = h1.text
            except:
                pass

            # ç­–ç•¥B (å…œåº•): ä»ç½‘é¡µæ ‡é¢˜æå–
            if name == "æœªçŸ¥åº”ç”¨" and "-" in current_title:
                name = current_title.split("-")[0].strip()
                print(f"ğŸ’¡ ä»æ ‡é¢˜æå–åˆ°åç§°: {name}")

            # æå–åº”ç”¨ç®€ä»‹
            desc = "æš‚æ— ç®€ä»‹"
            try:
                # ç­–ç•¥A: ä»metaæ ‡ç­¾æå–
                meta_desc = tab.ele('xpath://meta[@name="description"][2]', timeout=2)
                if meta_desc:
                    desc = meta_desc.attr('content').strip("<p>")

                # ç­–ç•¥B: ä»é¡µé¢å…ƒç´ æå–
                if not desc or len(desc) < 5:
                    desc_ele = tab.ele('css:.detail-description')
                    if desc_ele: 
                        desc = desc_ele.text
            except:
                pass

            # æ•´ç†æ•°æ®å¹¶ä¿å­˜
            data = {
                "name": name,
                "desc": desc[:150].replace('\n', ' '),  # æ¸…æ´—æ¢è¡Œç¬¦
                "url": url
            }

            self.save_to_csv(data, keyword)

        except Exception as e:
            print(f"âŒ å¤„ç†å¼‚å¸¸: {e}")
        finally:
            if tab:
                tab.close()

    def close(self):
        """å…³é—­æµè§ˆå™¨é¡µé¢"""
        self.page.quit()


if __name__ == "__main__":
    spider = None
    try:
        spider = LenovoSpider()

        # ç”¨æˆ·è¾“å…¥é…ç½®
        kw = input("è¯·è¾“å…¥å…³é”®è¯ï¼ˆé»˜è®¤ï¼šç”µè„‘ç®¡å®¶ï¼‰: ").strip() or "ç”µè„‘ç®¡å®¶"
        limit = input("è¯·è¾“å…¥çˆ¬å–æ•°é‡ï¼ˆé»˜è®¤ï¼š2ï¼‰: ").strip()
        limit = int(limit) if limit.isdigit() else 2

        spider.crawl_lenovo(kw, max_apps=limit)
        print(f"\nâœ¨ å…¨éƒ¨ä»»åŠ¡å®Œæˆï¼")

    except Exception as e:
        print(f"âŒ ç¨‹åºå¼‚å¸¸: {e}")
    finally:
        if spider:
            spider.close()